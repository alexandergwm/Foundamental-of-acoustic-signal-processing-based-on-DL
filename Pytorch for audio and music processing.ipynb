{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for pytorch for audio\n",
    "This jupyter notebook is used to store the code in pytorch for audio and music processing. Here is the original link:\n",
    "\n",
    "https://www.youtube.com/watch?v=gp2wZqDoJ1Y&list=PL-wATfeyAMNoirN4idjev6aRu8ISZYVWm&index=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST dataset downloaded\n",
      "Using cuda device\n",
      "Epoch 1\n",
      "Loss: 1.5076099634170532\n",
      "-----------------------\n",
      "Epoch 2\n",
      "Loss: 1.4944872856140137\n",
      "-----------------------\n",
      "Epoch 3\n",
      "Loss: 1.486358642578125\n",
      "-----------------------\n",
      "Epoch 4\n",
      "Loss: 1.4830163717269897\n",
      "-----------------------\n",
      "Epoch 5\n",
      "Loss: 1.4748579263687134\n",
      "-----------------------\n",
      "Epoch 6\n",
      "Loss: 1.4730068445205688\n",
      "-----------------------\n",
      "Epoch 7\n",
      "Loss: 1.4728569984436035\n",
      "-----------------------\n",
      "Epoch 8\n",
      "Loss: 1.4737215042114258\n",
      "-----------------------\n",
      "Epoch 9\n",
      "Loss: 1.473598599433899\n",
      "-----------------------\n",
      "Epoch 10\n",
      "Loss: 1.4722856283187866\n",
      "-----------------------\n",
      "Training is done\n",
      "Model trained and stored at feedforwardnet.pth\n"
     ]
    }
   ],
   "source": [
    "## 1. Train a feed forwarrd network\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = .001\n",
    "\n",
    "class FeedForwardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense_layers = nn.Sequential(\n",
    "            nn.Linear(28*28, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10)\n",
    "\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        flattened_data = self.flatten(input_data)\n",
    "        logits = self.dense_layers(flattened_data)\n",
    "        predictions = self.softmax(logits)\n",
    "        return predictions\n",
    "    \n",
    "\n",
    "# download datasets MNIST\n",
    "def download_mnist_datasets():\n",
    "    train_data = datasets.MNIST(\n",
    "        root=\"D:\\Coding\\Tutorial_DL_Acoustic_Signal\\DATA\\MNIST\",\n",
    "        download=True,\n",
    "        train=True,\n",
    "        transform = ToTensor()\n",
    "    )\n",
    "    validation_data = datasets.MNIST(\n",
    "    root=\"D:\\Coding\\Tutorial_DL_Acoustic_Signal\\DATA\\MNIST\",\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform = ToTensor()\n",
    "    )\n",
    "    return train_data, validation_data\n",
    "\n",
    "def train_one_epoch(model, data_loader, loss_fn, optimizer, device):\n",
    "    for inputs, targets in data_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # calculate loss\n",
    "        predictions = model(inputs)\n",
    "        loss = loss_fn(predictions, targets)\n",
    "\n",
    "        # backpropogate loss and update weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "def train(model, data_loader, loss_fn, optimizer, device, epochs):\n",
    "    for i in range(epochs):\n",
    "        print(f\"Epoch {i+1}\")\n",
    "        train_one_epoch(model, data_loader, loss_fn, optimizer, device)\n",
    "        print(\"-----------------------\")\n",
    "    print(\"Training is done\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # download MNIST dataset\n",
    "    train_data, _ = download_mnist_datasets()\n",
    "    print(\"MNIST dataset downloaded\")\n",
    "\n",
    "    # create a data loader for the train set\n",
    "    train_data_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "    # build model\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    print(f\"Using {device} device\")   \n",
    "    feed_forward_net = FeedForwardNet().to(device)\n",
    "\n",
    "    # instantiate loss function + optimizerr\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(feed_forward_net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # train model\n",
    "    train(feed_forward_net, train_data_loader, loss_fn, optimizer, device, EPOCHS)\n",
    "\n",
    "    # Store the model\n",
    "    torch.save(feed_forward_net.state_dict(), \"feedforwardnet.pth\")\n",
    "    print(\"Model trained and stored at feedforwardnet.pth\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: '7', expected :'7'\n"
     ]
    }
   ],
   "source": [
    "## 2. Making predictions/Inference\n",
    "import torch\n",
    "\n",
    "class_mapping = [\n",
    "    \"0\",\n",
    "    \"1\",\n",
    "    \"2\",\n",
    "    \"3\",\n",
    "    \"4\",\n",
    "    \"5\",\n",
    "    \"6\",\n",
    "    \"7\",\n",
    "    \"8\",\n",
    "    \"9\",\n",
    "]\n",
    "\n",
    "def predict(model, input, target, class_mapping):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(input)\n",
    "        # Tensor (1,10) -> [[0.1, 0.01,...,.0.6]]\n",
    "        predicted_index = predictions[0].argmax(0)\n",
    "        predicted = class_mapping[predicted_index]\n",
    "        expected = class_mapping[target]\n",
    "    return predicted, expected\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # load back the model\n",
    "    feed_forward_net = FeedForwardNet()\n",
    "    state_dict = torch.load(\"feedforwardnet.pth\")\n",
    "    feed_forward_net.load_state_dict(state_dict)\n",
    "\n",
    "    # load MNIST validation dataset\n",
    "    _, validation_data = download_mnist_datasets()\n",
    "\n",
    "    # get a sample from the validation dataset for inference\n",
    "    input, target = validation_data[0][0], validation_data[0][1]\n",
    "\n",
    "    # make an inference\n",
    "    predicted, expected = predict(feed_forward_net, input, target, class_mapping)\n",
    "\n",
    "    print(f\"Predicted: '{predicted}', expected :'{expected}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN for Classification of Urbansound 8k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      "There are 8732 samples in the dataset.\n",
      "torch.Size([1, 64, 44])\n"
     ]
    }
   ],
   "source": [
    "## Custome Datasets Urbandsound8k dataset\n",
    "## Transform the orginial signal into MelSpectrogram\n",
    "## Preprocessing audio with different durations\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "import os\n",
    "import torch\n",
    "\n",
    "class UrbanSoundDataset(Dataset):\n",
    "    def __init__(self, annotations_file, audio_dir, transformation, target_sample_rate, num_samples,device):\n",
    "        self.annotations = pd.read_csv(annotations_file)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.device = device\n",
    "        self.transformation = transformation.to(self.device)\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio_sample_path = self._get_audio_sample_path(index)\n",
    "        label = self._get_audio_sample_label(index)\n",
    "        signal, sr = torchaudio.load(audio_sample_path)\n",
    "        signal = signal.to(self.device)\n",
    "        # signal -> (num_channels, samples) -> (2, 16000) \n",
    "        signal = self._resample_if_necessary(signal, sr)\n",
    "        # signal -> (1, 16000)\n",
    "        signal = self._mix_down_if_necessary(signal)\n",
    "        signal = self._cut_if_necessary(signal)\n",
    "        signal = self._right_pad_if_necessary(signal)\n",
    "        signal = self.transformation(signal)\n",
    "        return signal, label\n",
    "    \n",
    "    def _cut_if_necessary(self, signal):\n",
    "        # signal -> Tensor -> (1, num_samples) -> (1, 50000) -> (0, 22050)\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:, :self.num_samples]\n",
    "        return signal\n",
    "    \n",
    "    def _right_pad_if_necessary(self, signal):\n",
    "        length_signal = signal.shape[1]\n",
    "        if length_signal < self.num_samples:\n",
    "            num_missing_samples = self.num_samples - length_signal\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            # (1, num_samples)\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal\n",
    "\n",
    "\n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate).to(self.device)\n",
    "            signal = resampler(signal) \n",
    "        return signal\n",
    "    \n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        if signal.shape[0] > 1:  # (2, 1000)\n",
    "            signal = torch.mean(signal,dim=0, keepdim=True)\n",
    "        return signal\n",
    "    \n",
    "    def _get_audio_sample_path(self, index):\n",
    "        fold = f\"fold{self.annotations.iloc[index, 5]}\"\n",
    "        path = os.path.join(self.audio_dir, fold, self.annotations.iloc[index, 0])\n",
    "        return path\n",
    "    \n",
    "    def _get_audio_sample_label(self, index):\n",
    "        return self.annotations.iloc[index, 6]\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    ANNOTATIONS_FILE = r\"D:\\Coding\\Tutorial_DL_Acoustic_Signal\\DATA\\UrbanSound8K\\UrbanSound8K\\metadata\\UrbanSound8K.csv\"\n",
    "    AUDIO_DIR = r\"D:\\Coding\\Tutorial_DL_Acoustic_Signal\\DATA\\UrbanSound8K\\UrbanSound8K\\audio\"\n",
    "    SAMPLE_RATE = 22050\n",
    "    NUM_SAMPLES = 22050\n",
    "    # The NUM_SAMPLES = SAMPLE_RATE  ->   1s Duration\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    print(f\"Using device {device}\")\n",
    "    \n",
    "    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate = SAMPLE_RATE,\n",
    "        n_fft = 1024,\n",
    "        hop_length = 512,\n",
    "        n_mels = 64\n",
    "    )\n",
    "    # mel_spectrogram(signal)\n",
    "    \n",
    "\n",
    "    usd = UrbanSoundDataset(ANNOTATIONS_FILE, AUDIO_DIR, mel_spectrogram, SAMPLE_RATE, NUM_SAMPLES,device)\n",
    "\n",
    "\n",
    "    print(f\"There are {len(usd)} samples in the dataset.\")\n",
    "\n",
    "    signal, label = usd[0]\n",
    "\n",
    "print(signal.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After pre-processing of the Urbandataset, the next step is to set a suitable model like CNN to classify the sound\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 66, 46]             160\n",
      "              ReLU-2           [-1, 16, 66, 46]               0\n",
      "         MaxPool2d-3           [-1, 16, 33, 23]               0\n",
      "            Conv2d-4           [-1, 32, 35, 25]           4,640\n",
      "              ReLU-5           [-1, 32, 35, 25]               0\n",
      "         MaxPool2d-6           [-1, 32, 17, 12]               0\n",
      "            Conv2d-7           [-1, 64, 19, 14]          18,496\n",
      "              ReLU-8           [-1, 64, 19, 14]               0\n",
      "         MaxPool2d-9             [-1, 64, 9, 7]               0\n",
      "           Conv2d-10           [-1, 128, 11, 9]          73,856\n",
      "             ReLU-11           [-1, 128, 11, 9]               0\n",
      "        MaxPool2d-12            [-1, 128, 5, 4]               0\n",
      "          Flatten-13                 [-1, 2560]               0\n",
      "           Linear-14                   [-1, 10]          25,610\n",
      "          Softmax-15                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 122,762\n",
      "Trainable params: 122,762\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.83\n",
      "Params size (MB): 0.47\n",
      "Estimated Total Size (MB): 2.31\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "class CNNNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Vgg network\n",
    "        # 4 conv blocks / flatten/ Linear / softmax\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1,padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1,padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1,padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1,padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(128*5*4, 10)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        x = self.conv1(input_data)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear(x)\n",
    "        predictions = self.softmax(logits)\n",
    "        return predictions\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cnn = CNNNetwork()\n",
    "    summary(cnn.cuda(), (1, 64, 44))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "CNNNetwork(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv4): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear): Linear(in_features=2560, out_features=10, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n",
      "Epoch 1\n",
      "Loss: 2.208237409591675\n",
      "-----------------------\n",
      "Epoch 2\n",
      "Loss: 2.134097099304199\n",
      "-----------------------\n",
      "Epoch 3\n",
      "Loss: 2.1360509395599365\n",
      "-----------------------\n",
      "Epoch 4\n",
      "Loss: 2.115302324295044\n",
      "-----------------------\n",
      "Epoch 5\n",
      "Loss: 2.026488780975342\n",
      "-----------------------\n",
      "Epoch 6\n",
      "Loss: 2.005101203918457\n",
      "-----------------------\n",
      "Epoch 7\n",
      "Loss: 2.0222995281219482\n",
      "-----------------------\n",
      "Epoch 8\n",
      "Loss: 1.9829366207122803\n",
      "-----------------------\n",
      "Epoch 9\n",
      "Loss: 1.946333646774292\n",
      "-----------------------\n",
      "Epoch 10\n",
      "Loss: 1.9948480129241943\n",
      "-----------------------\n",
      "Epoch 11\n",
      "Loss: 1.9780858755111694\n",
      "-----------------------\n",
      "Epoch 12\n",
      "Loss: 2.0029053688049316\n",
      "-----------------------\n",
      "Epoch 13\n",
      "Loss: 1.947679042816162\n",
      "-----------------------\n",
      "Epoch 14\n",
      "Loss: 1.9539227485656738\n",
      "-----------------------\n",
      "Epoch 15\n",
      "Loss: 1.9483543634414673\n",
      "-----------------------\n",
      "Epoch 16\n",
      "Loss: 1.9079859256744385\n",
      "-----------------------\n",
      "Epoch 17\n",
      "Loss: 1.9445871114730835\n",
      "-----------------------\n",
      "Epoch 18\n",
      "Loss: 1.907914638519287\n",
      "-----------------------\n",
      "Epoch 19\n",
      "Loss: 1.917872428894043\n",
      "-----------------------\n",
      "Epoch 20\n",
      "Loss: 1.9292141199111938\n",
      "-----------------------\n",
      "Epoch 21\n",
      "Loss: 1.908339023590088\n",
      "-----------------------\n",
      "Epoch 22\n",
      "Loss: 1.8736509084701538\n",
      "-----------------------\n",
      "Epoch 23\n",
      "Loss: 1.896833896636963\n",
      "-----------------------\n",
      "Epoch 24\n",
      "Loss: 1.8700493574142456\n",
      "-----------------------\n",
      "Epoch 25\n",
      "Loss: 1.8894567489624023\n",
      "-----------------------\n",
      "Epoch 26\n",
      "Loss: 1.8742014169692993\n",
      "-----------------------\n",
      "Epoch 27\n",
      "Loss: 1.9215185642242432\n",
      "-----------------------\n",
      "Epoch 28\n",
      "Loss: 1.8688580989837646\n",
      "-----------------------\n",
      "Epoch 29\n",
      "Loss: 1.9122997522354126\n",
      "-----------------------\n",
      "Epoch 30\n",
      "Loss: 1.8836675882339478\n",
      "-----------------------\n",
      "Epoch 31\n",
      "Loss: 1.894631028175354\n",
      "-----------------------\n",
      "Epoch 32\n",
      "Loss: 1.894598364830017\n",
      "-----------------------\n",
      "Epoch 33\n",
      "Loss: 1.8588287830352783\n",
      "-----------------------\n",
      "Epoch 34\n",
      "Loss: 1.847301721572876\n",
      "-----------------------\n",
      "Epoch 35\n",
      "Loss: 1.842432975769043\n",
      "-----------------------\n",
      "Epoch 36\n",
      "Loss: 1.8496763706207275\n",
      "-----------------------\n",
      "Epoch 37\n",
      "Loss: 1.879673719406128\n",
      "-----------------------\n",
      "Epoch 38\n",
      "Loss: 1.8631422519683838\n",
      "-----------------------\n",
      "Epoch 39\n",
      "Loss: 1.8737776279449463\n",
      "-----------------------\n",
      "Epoch 40\n",
      "Loss: 1.839600682258606\n",
      "-----------------------\n",
      "Training is done\n",
      "Model trained and stored at cnn.pth\n"
     ]
    }
   ],
   "source": [
    "## Train the model\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "EPOCHS = 40\n",
    "LEARNING_RATE = .0005\n",
    "ANNOTATIONS_FILE = r\"D:\\Coding\\Tutorial_DL_Acoustic_Signal\\DATA\\UrbanSound8K\\UrbanSound8K\\metadata\\UrbanSound8K.csv\"\n",
    "AUDIO_DIR = r\"D:\\Coding\\Tutorial_DL_Acoustic_Signal\\DATA\\UrbanSound8K\\UrbanSound8K\\audio\"\n",
    "SAMPLE_RATE = 22050\n",
    "NUM_SAMPLES = 22050\n",
    "\n",
    "\n",
    "def create_data_loader(train_data, batch_size):\n",
    "    train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "    return train_dataloader\n",
    "\n",
    "def train_one_epoch(model, data_loader, loss_fn, optimizer, device):\n",
    "    for inputs, targets in data_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # calculate loss\n",
    "        predictions = model(inputs)\n",
    "        loss = loss_fn(predictions, targets)\n",
    "\n",
    "        # backpropogate loss and update weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "def train(model, data_loader, loss_fn, optimizer, device, epochs):\n",
    "    for i in range(epochs):\n",
    "        print(f\"Epoch {i+1}\")\n",
    "        train_one_epoch(model, data_loader, loss_fn, optimizer, device)\n",
    "        print(\"-----------------------\")\n",
    "    print(\"Training is done\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    print(f\"Using {device}\")\n",
    "\n",
    "    # instantiating our dataset object and crreate data loader\n",
    "    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate = SAMPLE_RATE,\n",
    "        n_fft = 1024,\n",
    "        hop_length = 512,\n",
    "        n_mels = 64\n",
    "    )\n",
    "    \n",
    "    usd = UrbanSoundDataset(ANNOTATIONS_FILE, AUDIO_DIR, mel_spectrogram, SAMPLE_RATE, NUM_SAMPLES,device)\n",
    "    \n",
    "    train_dataloader = create_data_loader(usd, BATCH_SIZE)\n",
    "\n",
    "    # build model\n",
    "    cnn = CNNNetwork().to(device)\n",
    "    print(cnn)\n",
    "\n",
    "    # instantiate loss function + optimizerr\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(cnn.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # train model\n",
    "    train(cnn, train_dataloader, loss_fn, optimizer, device, EPOCHS)\n",
    "\n",
    "    # Store the model\n",
    "    torch.save(cnn.state_dict(), \"cnn.pth\")\n",
    "    print(\"Model trained and stored at cnn.pth\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 'dog_bark', expected :'dog_bark'\n"
     ]
    }
   ],
   "source": [
    "## 2. Making predictions/Inference\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "class_mapping = [\n",
    "    \"air_conditioner\",\n",
    "    \"car_horn\",\n",
    "    \"children_playing\",\n",
    "    \"dog_bark\",\n",
    "    \"drilling\",\n",
    "    \"engine_idling\",\n",
    "    \"gun_shot\",\n",
    "    \"jackhammer\",\n",
    "    \"siren\",\n",
    "    \"street_music\",\n",
    "]\n",
    "\n",
    "def predict(model, input, target, class_mapping):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(input)\n",
    "        # Tensor (1,10) -> [[0.1, 0.01,...,.0.6]]\n",
    "        predicted_index = predictions[0].argmax(0)\n",
    "        predicted = class_mapping[predicted_index]\n",
    "        expected = class_mapping[target]\n",
    "    return predicted, expected\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # load back the model\n",
    "    cnn = CNNNetwork()\n",
    "    state_dict = torch.load(\"cnnnet.pth\")\n",
    "    cnn.load_state_dict(state_dict)\n",
    "\n",
    "    # load Urban sound dataset\n",
    "    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate = SAMPLE_RATE,\n",
    "        n_fft = 1024,\n",
    "        hop_length = 512,\n",
    "        n_mels = 64\n",
    "    )\n",
    "    \n",
    "    usd = UrbanSoundDataset(ANNOTATIONS_FILE, AUDIO_DIR, mel_spectrogram, SAMPLE_RATE, NUM_SAMPLES, \"cpu\")\n",
    "    \n",
    "    # get a sample from the urban sound dataset for inference\n",
    "    input, target = usd[0][0], usd[0][1]\n",
    "    input.unsqueeze_(0)\n",
    "    # make an inference\n",
    "    predicted, expected = predict(cnn, input, target, class_mapping)\n",
    "\n",
    "    print(f\"Predicted: '{predicted}', expected :'{expected}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[8.1443e-04, 2.1588e-04, 9.1436e-04,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [2.4870e-03, 1.5724e-03, 4.2855e-04,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [2.9685e-03, 6.0449e-03, 4.1659e-03,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         ...,\n",
       "         [1.7061e-04, 5.7996e-02, 7.9969e-01,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [2.1515e-04, 1.5781e-02, 3.7936e-01,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [3.0015e-04, 1.4416e-02, 3.1233e-01,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
